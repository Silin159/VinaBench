load_from: ./OUTPUT/mm_interleaved_pretrain

# Training Arguments

fp16: True
# max_steps: 15_000
max_steps: 40_000
# max_steps: 4_000
# max_steps: 400
# resume: true
per_device_train_batch_size: &per_device_train_batch_size 2
per_device_eval_batch_size: 1
dataloader_num_workers: &num_workers 4
data_seed: &data_seed 0
seed: 32

## optimizer & scheduler

optim: adamw_torch
# learning_rate: 1.0e-4
learning_rate: 1.0e-5
# learning_rate: 5.0e-6
# learning_rate: 2.0e-5
weight_decay: 0.05
# adam_beta1: 0.9
# adam_beta2: 0.995
# adam_epsilon: 1.0e-6
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1.0e-8
# lr_for_random_params_list: [1.0e-4, 1.0e-5, 1.0e-4, 1.0e-5]
lr_for_random_params_list: [1.0e-5, 1.0e-4, 1.0e-5, 1.0e-4]
# lr_for_random_params_list: [5.0e-6, 5.0e-5, 5.0e-6, 5.0e-5]
# lr_for_random_params_list: [2.0e-5, 2.0e-4, 2.0e-5, 2.0e-4]
wd_for_random_params_list: [0.0, 0.0, null, null]
random_params_list: [llama_cross_attn.gate, sampling_offsets, llama_cross_attn, image_decoder.decoder.unet]

lr_scheduler_type: "cosine"
# warmup_steps: 200
# warmup_steps: 400
# warmup_steps: 1_000
warmup_steps: 2_000

## evaluation & saving

# evaluation_strategy: "no"
evaluation_strategy: "steps"
eval_steps: 2_000
save_strategy: "steps"
save_steps: 2_000
save_total_limit: 20
fp16_full_eval: false

generate_mode: generate_both  # not used

## logging

report_to: ['wandb']
# report_to: ['tensorboard']
logging_steps: 10
disable_tqdm: False
log_level: info

## misc

tf32: True
ddp_find_unused_parameters: False

## deepspeed

deepspeed: './mm_interleaved/configs/release/deepspeed_zero1.json'


# MODEL

model:
  llm_model_path: &tokenizer_path lmsys/vicuna-13b-v1.3
  num_img_token: &img_len 64
  cross_attention_frequency: 4

  dataset_to_ignore_noimage_cond_loss: [laion_en, laion_coco]

  visual_tokenizer_config:
    encoder_model_path: openai/clip-vit-large-patch14
    perceiver_config:
      num_queries: 64
      hidden_size: 768
      encoder_hidden_size: 1024
      cross_attention_frequency: 2
      num_hidden_layers: 12
      num_attention_heads: 12
      qk_normalization: True
  image_decoder_config:
    pretrained_model_name_or_path: 'stabilityai/stable-diffusion-2-1-base'
    sd_base_seed: 0
    sd_use_random_seed: False
    perceiver_config:
      num_queries: 77
      hidden_size: 1024
      encoder_hidden_size: 5120
      cross_attention_frequency: 1
      num_hidden_layers: 1
      num_attention_heads: 16
      hidden_dropout_prob: 0.
      attention_probs_dropout_prob: 0.

# DATA
# out_mode: images | captions_links_setups_images
# for data.val.annt_root, we suggest to test on a subset (e.g., 10 samples) of salon_short_test.json to save time
out_mode: captions_links_setups_images
data:
  train:
    name: storysalon
    data_root: "../data/Image_inpainted"
    annt_root: "../data/annotations/salon_short_train.json"
    phase: train
    collator: MultiImageCollator
    tokenizer_path: *tokenizer_path
    collate_mode: train
    out_mode: ${out_mode}
    num_img_token: *img_len
    context_type: multi_modal
    transform:
      aug_type: 'dual_numpy'
      resolution: 224
      resolution2: 512
      hw_ratio: 1.0

  val:
    name: storysalon
    data_root: "../data/Image_inpainted"
    annt_root: "../data/annotations/salon_short_test_10.json"
    phase: val
    collator: MultiImageCollator
    tokenizer_path: *tokenizer_path
    collate_mode: generate_images
    out_mode: ${out_mode}
    num_img_token: *img_len
    context_type: multi_modal
    target_image_idxs: all
    transform:
      aug_type: 'dual_numpy'
      resolution: 224
      resolution2: 512
      hw_ratio: 1.0
