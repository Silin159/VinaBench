load_from: ./OUTPUT/mm_interl_ft_vwp_with_cons_40k/checkpoint-30000/pytorch_model.bin
data_root: ../data/images
annt_path: ../data/annotations/vwp_test.json
# can test on subset samples
start: 0
end: 849
output_dir: ./OUTPUT/vwp_inf/gold_cons_ds250_full_inf_30k

# MODEL

model:
  llm_model_path: &tokenizer_path lmsys/vicuna-13b-v1.3
  num_img_token: &img_len 64

  visual_tokenizer_config:
    encoder_model_path: openai/clip-vit-large-patch14
    perceiver_config:
      num_queries: 64
      hidden_size: 768
      encoder_hidden_size: 1024
      cross_attention_frequency: 2
      num_hidden_layers: 12
      num_attention_heads: 12
      qk_normalization: True
  image_decoder_config:
    pretrained_model_name_or_path: stabilityai/stable-diffusion-2-1-base
    sd_base_seed: 42
    perceiver_config:
      num_queries: 77
      hidden_size: 1024
      encoder_hidden_size: 5120
      cross_attention_frequency: 1
      num_hidden_layers: 1
      num_attention_heads: 16
      hidden_dropout_prob: 0.
      attention_probs_dropout_prob: 0.

# INFERENCE

inference:
  tokenizer_path: *tokenizer_path
  num_img_token: *img_len
  generate_mode: generate_both

  transform:
    aug_type: 'dual_numpy_pil'
    resolution: 224
    resolution2: 512
    hw_ratio: 0.4375

  generation_kwargs:
    max_length: 200
    min_length: 8
    num_beams: 1
    use_nucleus_sampling: True
    repetition_penalty: 1.3
    guidance_scale: 7.5
    num_inference_steps: 250
    num_validation_images: 1

